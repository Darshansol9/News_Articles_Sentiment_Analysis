{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from newspaper import Article\n",
    "from newspaper import fulltext\n",
    "import os \n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnn():\n",
    "    \n",
    "    #Changing the directory where to place the site_maps files\n",
    "    os.chdir(r'C:\\Users\\Darshan\\Music\\raman_kannan\\NLP\\site_maps')\n",
    "    \n",
    "    #CNN has started using sitemaps from 2011 7th month so extracted from 2012 and explicitly done for 2011 done with this script\n",
    "    \n",
    "    years = ['2012','2013','2014','2015','2016','2017','2018','2019']\n",
    "    months = ['01','02','03','04','05','06','07','08','09','10','11','12']\n",
    "    url_main = 'https://www.cnn.com/us/articles/sitemap-'\n",
    "    for y in years:\n",
    "        for m in months:\n",
    "            url  = url_main + f'{y}-{m}.html'\n",
    "            print(url)\n",
    "            try:\n",
    "                page = requests.get(url=url,verify=True)\n",
    "                \n",
    "                #Checking the status of the requested page 200 means All ok\n",
    "                if(page.status_code == 200):\n",
    "\n",
    "                    # Extracting atricle links using Beautiful soup library\n",
    "                    #The given urls viewed in page source where embedded in class call 'sitemap-link'\n",
    "\n",
    "                    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "                    links = soup.find_all(class_ = 'sitemap-link')\n",
    "                    \n",
    "\n",
    "                    urls = {'url':[],'title':[]}\n",
    "                    count = 1  #Ignoring the first link as it does not contains 'a' tag\n",
    "                    for link in links:\n",
    "\n",
    "                        lk = link.find('a',href=True)\n",
    "                        if(count==1):\n",
    "                            count+=1\n",
    "                            continue  \n",
    "                            \n",
    "                        url_ = lk['href']\n",
    "                        text = lk.get_text()\n",
    "                        urls['url'].append(url_)\n",
    "                        urls['title'].append(text)\n",
    "\n",
    "                    #Storing in json frame\n",
    "                    df = pd.DataFrame(urls)\n",
    "                    df.to_json(f'Sitemap-{y}-{m}.json')\n",
    "                    \n",
    "                    #Sleeping for 2 seconds to avoid overloading the server with the requests\n",
    "                    time.sleep(2)\n",
    "                    \n",
    "                except Exceptions as e:\n",
    "                    print(str(e))\n",
    "cnn()\n",
    "print('Request Completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cleaning sitemap files with containing duplicate articles\n",
    "\n",
    "def cleaningSiteMaps():\n",
    "    \n",
    "    os.chdir(r'C:\\Users\\Darshan\\Music\\raman_kannan\\NLP\\site_maps_copy')\n",
    "    \n",
    "    for file_name in os.listdir('.'):\n",
    "        file_n,file_ext = os.path.splitext(file_name)\n",
    "        df = pd.read_json(file_name)\n",
    "        df.reset_index(drop=True,inplace=True)\n",
    "        df.drop_duplicates(subset='title',keep='first',inplace=True)\n",
    "        df.to_json(f'{file_n}-pp{file_ext}')\n",
    "        os.remove(file_name)\n",
    "\n",
    "cleaningSiteMaps()\n",
    "\n",
    "#print('Request Completed')\n",
    "    \n",
    "\n",
    "#To check how many urls and titles are getting duplicated.\n",
    "\n",
    "#print(len(df['url']),len(df['title']))\n",
    "#print(len(df['url'].unique()),len(df['title'].unique()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
