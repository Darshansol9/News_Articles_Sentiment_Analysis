{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from newspaper import Article\n",
    "from newspaper import fulltext\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnn():\n",
    "    years = ['2017','2018','2019']\n",
    "    months = ['01','02','03','04','05','06','07','08','09','10','11','12']\n",
    "    url = 'https://www.cnn.com/article/sitemap-'\n",
    "    for y in years:\n",
    "        for m in months:\n",
    "            url  = url + f'{y}-{m}.html'\n",
    "            page = requests.get(url=url,verify=True)\n",
    "            \n",
    "            # Extracting things using Beautiful soup libraries\n",
    "            #The given urls viewed in page source where embedded in class call 'sitemap-link'\n",
    "            \n",
    "            soup = BeautifulSoup(page.content, 'html.parser')\n",
    "            links = soup.find_all('span',class_ ='sitemap-link')\n",
    "            urls = {'url':[],'title':[]}\n",
    "            count = 1 #Ignoring the first link as it does not contains 'a' tag\n",
    "            for link in links:\n",
    "                \n",
    "                lk = link.find('a',href=True)\n",
    "                if(count==1):\n",
    "                    count+=1\n",
    "                    continue  \n",
    "                url_ = lk['href']\n",
    "                text = lk.get_text()\n",
    "                urls['url'].append(url_)\n",
    "                urls['title'].append(text)\n",
    "            \n",
    "            #Storing in json frame\n",
    "            df = pd.DataFrame(urls)\n",
    "            df.to_json(f'Sitemap-{y}-{m}'.json\")\n",
    "            #print(\"Request completed\")\n",
    "#cnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cleaning sitemap files with containing duplicate articles\n",
    "\n",
    "for file_name in links:\n",
    "    file_n,file_ext = os.path.splitext(file_name)\n",
    "    df = pd.read_json(file_name)\n",
    "    df.reset_index(drop=True,inplace=True)\n",
    "    df.drop_duplicates(subset='title',keep='first',inplace=True)\n",
    "    df.to_json(f'{file_n}_processed.{file_ext}')\n",
    "\n",
    "print('Request Completed')\n",
    "    \n",
    "\n",
    "#To check how many urls and titles are getting duplicated.\n",
    "#print(len(df['url']),len(df['title']))\n",
    "#print(len(df['url'].unique()),len(df['title'].unique()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
