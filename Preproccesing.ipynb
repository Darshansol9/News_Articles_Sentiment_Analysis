{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_text(texts):\n",
    "    processed_sentences = []\n",
    "    for text in sent_tokenize(texts):\n",
    "        \n",
    "        #Removing all special characters'\n",
    "        processed_feature = re.sub(r'\\W', ' ', text)\n",
    "\n",
    "        # remove all single characters\n",
    "        processed_feature= re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_feature)\n",
    "\n",
    "        # Remove single characters from the start\n",
    "        processed_feature = re.sub(r'\\^[a-zA-Z]\\s+', ' ', processed_feature) \n",
    "\n",
    "        # Substituting multiple spaces with single space\n",
    "        processed_feature = re.sub(r'\\s+', ' ', processed_feature, flags=re.I)\n",
    "\n",
    "        # Converting to Lowercase\n",
    "        processed_feature = processed_feature.lower()\n",
    "\n",
    "        # Storing each processed sent in the list\n",
    "        processed_sentences.append(processed_feature)\n",
    "        \n",
    "    processed_sent = '.'.join(processed_sentences)\n",
    "    #print(processed_sent)\n",
    "    return processed_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Removing stopwords\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    #Using stopwords library\n",
    "    \n",
    "    stop_words_ = set(stopwords.words('english'))\n",
    "    filter_stopwords = []\n",
    "    \n",
    "    for sent in sentences:\n",
    "        words = word_tokenize(sent)\n",
    "        filtered_words = []\n",
    "        for w in words:\n",
    "            if(w not in stop_words_):\n",
    "                filtered_words.append(w)\n",
    "\n",
    "        sentence =' '.join(filtered_words)\n",
    "        filter_stopwords.append(sentence)\n",
    "        \n",
    "    sent = '.'.join(filter_stopwords)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Lemmatizing the words\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    processed_features = []\n",
    "    filter_stopwords = sent_tokenize(text)\n",
    "    \n",
    "    \n",
    "    for sent in filter_stopwords:\n",
    "        words = word_tokenize(sent)\n",
    "        filter_lemma = []\n",
    "        for w in words:\n",
    "            filter_lemma.append(lemmatizer.lemmatize(w))\n",
    "\n",
    "        sentence = ' '.join(filter_lemma)\n",
    "        processed_features.append(sentence)\n",
    "        \n",
    "    processed_text = '.'.join(processed_features)\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0CvZyqGK83qdAxAOV70YIiASXgbPqF2y.txt processsing 52.88035450516987\n",
      "0KU7TdRKtkfn95WcXoUNIbO1yezJImvX.txt processsing 47.63513513513514\n",
      "0lZh3ECMrJxRQh5OwATOb1zZB9wpDo06.txt processsing 53.57142857142857\n",
      "0OPAeMErixbhjOn86FgsRiimaVMXaz6H.txt processsing 47.89473684210526\n",
      "1dGIFt0ooHbVviopqyRwlmm1r9wmL6Vh.txt processsing 46.63865546218487\n",
      "1i5echWDRemeDDOJ3zEUkxV6utc9cOCB.txt processsing 62.5\n",
      "1MhpOzsIjbS5Dx0dKuLWgbM1sQlimtg8.txt processsing 42.76315789473684\n",
      "1x6omVvrJssNdgDHvwXPqfHqlWWJF8aV.txt processsing 42.025862068965516\n",
      "2EMMlzwAfTv37RwAQv0rHVJdzFLhhWPS.txt processsing 54.377880184331794\n",
      "2FrS9SQX9mNIPcoJq7igCrAARR9PpvWe.txt processsing 46.835443037974684\n",
      "3IeGXabA2snLF339czu9m2FPsUp0OnUf.txt processsing 46.0\n",
      "3toCnxW6bb0ofCZ1Y1DgJvszyqSD5Ja6.txt processsing 53.84615384615385\n",
      "4nCHet4c47P7UKgrRxuCrqi5k9PK4nH8.txt processsing 52.610837438423644\n",
      "4wwU8TfILcAEtFsThhHBhPRGzVDOH2P1.txt processsing 49.171270718232044\n",
      "5ld7XSeD7whQo15NbSglh1aU3By9VxAL.txt processsing 49.445865302642794\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-0ac4c31ad25d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;31m#Reading data from the file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mlst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mtexts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlst\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[1;31m#print(texts)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "os.chdir(r'C:\\Users\\Darshan\\Music\\raman_kannan\\NLP\\preprocessing')\n",
    "\n",
    "for file in os.listdir('.'):\n",
    "    if(file.find('.txt') and file is not 'kmeY26maA1WXmrb0Pc3I1C4sMfiPqDAZ.txt'):\n",
    "        #Spilting the basename and extension\n",
    "        file_n,file_ext = os.path.splitext(file)\n",
    "\n",
    "        #Reading data from the file\n",
    "        lst = open(file,'r',encoding='utf-8').readlines()\n",
    "        if(len(lst[0]) == 0):\n",
    "            continue\n",
    "        texts = str(lst[0])\n",
    "        \n",
    "        #print(texts)\n",
    "        \n",
    "        #Saving the number of words in original file to compare with preprocessed file words\n",
    "        original_sent = word_tokenize(texts)\n",
    "\n",
    "        #Cleaning the text\n",
    "        clean_sent = preprocess_text(texts)\n",
    "        #print('Cleaning words Done')\n",
    "       \n",
    "        \n",
    "        #Removing stopwords\n",
    "        clean_stop = remove_stopwords(clean_sent)\n",
    "        #print('Cleaning stop words Done')\n",
    "        \n",
    "        #Lemmatizing the words\n",
    "        clean_lemma = lemmatize_text(clean_stop)\n",
    "        #print('Cleaning lemma words Done')\n",
    "        \n",
    "        #Giving the comparision in percentage of words reduced\n",
    "        print(f'{file} processsing',(len(original_sent)-len(word_tokenize(clean_lemma)))/len(original_sent)*100)\n",
    "        \n",
    "        clean_file = preprocess_text(clean_lemma)\n",
    "        \n",
    "        #Writing it in a file\n",
    "        fp = open(f'{file_n}_pp{file_ext}','w')\n",
    "        fp.write(clean_file)\n",
    "        fp.close()\n",
    "        \n",
    "        #Deleting original file\n",
    "        \n",
    "        #os.remove(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
